from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import time
import os
import prox_tv
from process_constants import *
from phantom_generator import *
# Generate the the first part of the first part of the ACR Phantom
materials = {
    "bone":        955.0,
    "poly":       -95.0,
    "acrylic":    120.0,
    "air":      -1000.0,
    "water":    0.0
}
materials_base = {
    "air":      -1000.0,
    "water":    0.0
}

nm = len(materials_base)+1

gpus = [0]
phantom_size = 512
base_slice, target_slice = iodine_phantom(phantom_size)
x_true = np.zeros((*target_slice.shape, nm), dtype = np.float32)
diff = target_slice - base_slice

for i, (material, value) in enumerate(materials_base.items()):
    x_true[..., i] = (base_slice == value) 
x_bar = x_true.copy()
diff /= diff.max()
diff *= 0.0025
x_true[...,-1] = diff
print(f'current size {target_slice.shape}')
if phantom_size < 30:
    step_size = 1e-5#5e-10
elif phantom_size < 50:
    step_size = 1e23#1e-9
elif phantom_size < 60:
    step_size = 2e-5#3.5e-9
elif phantom_size < 70:
    step_size = 1e-3
elif phantom_size < 80:
    step_size = 3e-5
elif phantom_size < 130:
    step_size = 1e-6  #1e-5
elif phantom_size < 270:
    step_size = 5e-5#6.2e-8#6.2e-7
elif phantom_size < 530:
    step_size = 5e-2#3.3e-4 #monotone: 5e-4
elif phantom_size < 1040:
    step_size = 1e-3#6.2e-8#6.2e-7
else:
    step_size = 1e-7
stepsizes = np.array([step_size])
print('step size', stepsizes)
nm_algorithm = 1
p_tv = 0.1545
# parameters for the scanning setup
nx, ny = target_slice.shape # dimension of image
scanner_rad = 30 # radius from center to gantry (cm)
image_side = 10 # length of one side of square image array (cm)
theta = 4*np.arcsin(image_side/(np.sqrt(2.0)*scanner_rad)) # angle of the beam (radians)
phantom_rad = 5 # radius of the circular phantom (cm)
nk = nx*ny  # number of pixels
energies = np.arange(1,100,2); ni = len(energies); energy_binwidth=2 # discretizing photon energy (keV)
nw = 3 # number of energy windows in detector

# parameters for optimization with ADMM
sig_grid = np.array([100]) # This used to be np.array([1,10,100]) # ADMM parameter
nsig = len(sig_grid)
div_by_zero_thresh = 1e-8 # avoid dividing by zero with step size matrices
niter_newton = 10 # Newton-Raphson algorithm for solving convex subproblem


# parameters for optimization with monotone operator or extragradient method
nstepsize = len(stepsizes)
polyakstepsizes = np.array([1])
npolyakstepsize = len(polyakstepsizes)
gdstepsizes = np.array([5e-9])
ngdstepsize = len(gdstepsizes)


# create source/detector response S

# create window functions with linear blur
blur_prop = 0.1 # width of blur on each side as a function of width of total range
blur_width = blur_prop*(energies.max() - energies.min())/nw
w_breaks = np.r_[1.,50.,70.,99.]
w_breaks[0] -= blur_width
w_breaks[-1] += blur_width
w_shape = np.zeros((nw,ni))

spectrum = np.c_[np.arange(10,102,2), np.array([0.00368,0.00355,0.00459,0.0077,0.01474,\
  0.02249,0.02691,0.03264,0.03889,0.04088,0.04307,0.04396,0.04428,0.04315,0.04284,0.04095,\
  0.03944,0.03795,0.03627,0.0346,0.03267,0.03113,0.02894,0.02733,0.02647,0.05175,0.02204,\
  0.02084,0.01866,0.02607,0.01502,0.01218,0.01121,0.01026,0.0092,0.00854,0.00749,0.00682,\
  0.00579,0.00486,0.00417,0.00324,0.0024,0.00168,0.00114,0.00017])]
# These values are taken from the following paper:
# "Diagnostic x-ray spectra: A comparison of spectra generated by
#     different computational methods with a measured spectrum"
# Bhat et al, Med. Phys. 25(1), 1998
# (Table I)

spectrum = np.exp(np.interp(energies, spectrum[:,0], np.log(spectrum[:,1]), \
                            left = -np.inf, right = -np.inf))
spectrum /= spectrum.sum()
# print(spectrum.shape)
for w_ in range(nw):
    w_shape[w_] = \
      (1 - np.minimum(np.maximum((w_breaks[w_] + blur_width - energies)/2/blur_width,0),1)\
       - np.minimum(np.maximum((energies - w_breaks[w_+1] + blur_width)/2/blur_width,0),1))\
        * spectrum

def get_S(total_intensity, nl):
    S = total_intensity * np.outer(w_shape,np.ones(nl)).reshape((nw,ni,nl)).transpose((0,2,1))
    # constant across rays l_=1,..,nl i.e. no differences between different detector cells
    S = S[...,:]
    print(S.shape)
    return S

def get_S(total_intensity, nl):
    """
    intensity_per_ray : (nl,) 
    nl                : total ray
    return            : (nw, nl, ni)
    """
    intensity_per_ray = np.asarray(total_intensity, dtype=np.float32)
    # print(intensity_per_ray.shape, nl)
    if intensity_per_ray.ndim == 0:
        intensity_per_ray = np.full(nl, intensity_per_ray, dtype=np.float32)
    elif intensity_per_ray.size != nl:
        raise ValueError("length of intensity_per_ray must equal nl")

    # w_shape : (nw, ni) -> broadcast -> (nw, ni, nl) -> transpose
    S = (w_shape[:, :, None] * intensity_per_ray[None, None, :]).transpose(0, 2, 1)
    # print(S.shape)
    return S.astype(np.float32)

def plot_detector_spectra(total_intensity, nl):
    S = get_S(total_intensity=total_intensity, nl=nl)
    plt.figure(figsize=(10, 3))
    plt.subplot(121)
    plt.plot(energies,spectrum/energy_binwidth,label='X-ray beam') # rescaling to obtain a density
    plt.legend(loc='upper right')
    plt.xlabel('Energy (keV)')
    plt.ylabel('Photon Spectral Density')
    plt.subplot(122)
    plt.plot(energies[len(energies)-S.shape[-1]:],S[0,0]/total_intensity/energy_binwidth,label='Window 1')
    plt.plot(energies[len(energies)-S.shape[-1]:],S[1,0]/total_intensity/energy_binwidth,label='Window 2')
    plt.plot(energies[len(energies)-S.shape[-1]:],S[2,0]/total_intensity/energy_binwidth,label='Window 3')
    plt.legend(loc='upper right')
    plt.xlabel('Energy (keV)')
    plt.yticks([])
    plt.tight_layout()
    plt.savefig('detector_spectra.jpg')
    plt.close()


density_bone, mu_per_density_bone = 1.920, mu_generator(bone_data)
mu_bone = density_bone * mu_per_density_bone
density_poly, mu_per_density_poly = 0.930, mu_generator(poly_data)
mu_poly = density_poly * mu_per_density_poly
density_acrylic, mu_per_density_acrylic = 1.190, mu_generator(acrylic_data)
mu_acrylic = density_acrylic * mu_per_density_acrylic
density_iodine, mu_per_density_iodine = 4.93, mu_generator(iodine_data)
mu_iodine = density_iodine * mu_per_density_iodine


density_PMMA = 1.190
mu_per_density_PMMA = np.array([\
        2.79400000e+03, 1.23600000e+02, 2.68100000e+01, 1.00166012e+01,\
        4.66908535e+00, 2.68607749e+00, 1.71970094e+00, 1.10100000e+00,\
        8.46931890e-01, 6.51492848e-01, 5.36313998e-01, 4.72472923e-01,\
        4.16231282e-01, 3.66684463e-01, 3.23035537e-01, 2.95571846e-01,\
        2.80886455e-01, 2.66930703e-01, 2.53668338e-01, 2.41064909e-01,\
        2.32082263e-01, 2.26355019e-01, 2.20769110e-01, 2.15321048e-01,\
        2.10007431e-01, 2.05848821e-01, 2.02781180e-01, 1.99759255e-01,\
        1.96782364e-01, 1.93849835e-01, 1.91495742e-01, 1.89699956e-01,\
        1.87921010e-01, 1.86158746e-01, 1.84413009e-01, 1.82683642e-01,\
        1.80970493e-01, 1.79273409e-01, 1.77592240e-01, 1.75926836e-01,\
        1.74532885e-01, 1.73404160e-01, 1.72282734e-01, 1.71168560e-01,\
        1.70061593e-01, 1.68961784e-01, 1.67869087e-01, 1.66783457e-01,\
        1.65704848e-01, 1.64633215e-01])
mu_PMMA = mu_per_density_PMMA * density_PMMA

density_aluminum = 2.699
mu_per_density_aluminum = np.array([\
        1.18500000e+03, 7.88000000e+02, 1.93400000e+02, 7.61777461e+01,\
        3.63339497e+01, 2.06617294e+01, 1.28204546e+01, 7.95500000e+00,\
        5.68928879e+00, 4.06888837e+00, 3.07784762e+00, 2.46247597e+00,\
        1.97013908e+00, 1.57623792e+00, 1.26109167e+00, 1.05329794e+00,\
        9.18407688e-01, 8.00792108e-01, 6.98238929e-01, 6.08819189e-01,\
        5.44319639e-01, 4.99000607e-01, 4.57454752e-01, 4.19367927e-01,\
        3.84452140e-01, 3.57884146e-01, 3.38295129e-01, 3.19778329e-01,\
        3.02275059e-01, 2.85729841e-01, 2.73395705e-01, 2.64795487e-01,\
        2.56465807e-01, 2.48398154e-01, 2.40584285e-01, 2.33016217e-01,\
        2.25686218e-01, 2.18586800e-01, 2.11710708e-01, 2.05050917e-01,\
        2.00100689e-01, 1.96744874e-01, 1.93445339e-01, 1.90201138e-01,\
        1.87011345e-01, 1.83875047e-01, 1.80791347e-01, 1.77759362e-01,\
        1.74778226e-01, 1.71847085e-01])
mu_aluminum = mu_per_density_aluminum * density_aluminum

density_gadolinium = 7.900
mu_per_density_gadolinium = np.array([\
        2291.        , 1292.        ,  365.3       ,  156.89085391,\
        353.5249706 ,  217.87728117,  142.61431975,   93.35      ,\
         68.86248306,   50.79851713,   39.16968525,   31.57038073,\
         25.44541609,   20.50875489,   16.52985455,   13.74995004,\
         11.80417281,   10.13374561,    8.6997032 ,    7.46859441,\
          6.52744112,    5.8078675 ,    5.16761841,    4.59794925,\
          4.09107942,   17.98139263,   16.35911816,   14.88320467,\
         13.54044755,   12.31883346,   11.31984357,   10.50619708,\
          9.75103377,    9.05015002,    8.39964431,    7.79589558,\
          7.23554304,    6.71546746,    6.23277381,    5.78477517,\
          5.41272081,    5.10585873,    4.81639351,    4.54333887,\
          4.28576446,    4.04279264,    3.81359557,    3.59739231,\
          3.39344622,    3.20106238])
mu_gadolinium = mu_per_density_gadolinium * density_gadolinium

density_air = 1.205e-3
mu_per_density_air = np.array([\
        3.60600000e+03, 1.62500000e+02, 4.02700000e+01, 1.52397707e+01,\
        7.12709759e+00, 4.06440217e+00, 2.56123898e+00, 1.61400000e+00,\
        1.20534571e+00, 9.00160026e-01, 7.18964053e-01, 6.14149339e-01,\
        5.24615116e-01, 4.48133707e-01, 3.82802198e-01, 3.41518854e-01,\
        3.18220679e-01, 2.96511888e-01, 2.76284056e-01, 2.57436153e-01,\
        2.44118160e-01, 2.35584914e-01, 2.27349951e-01, 2.19402844e-01,\
        2.11733531e-01, 2.05852966e-01, 2.01625156e-01, 1.97484177e-01,\
        1.93428245e-01, 1.89455614e-01, 1.86372898e-01, 1.84138980e-01,\
        1.81931839e-01, 1.79751153e-01, 1.77596605e-01, 1.75467882e-01,\
        1.73364674e-01, 1.71286676e-01, 1.69233586e-01, 1.67205105e-01,\
        1.65573031e-01, 1.64326181e-01, 1.63088720e-01, 1.61860578e-01,\
        1.60641684e-01, 1.59431969e-01, 1.58231364e-01, 1.57039800e-01,\
        1.55857209e-01, 1.54683524e-01])
mu_air = mu_per_density_air * density_air

density_water = 1.0
mu_per_density_water = np.array([\
        4.07800000e+03, 1.92900000e+02, 4.25800000e+01, 1.59848929e+01,\
        7.43382338e+00, 4.22683957e+00, 2.65922970e+00, 1.67300000e+00,\
        1.25142770e+00, 9.36085650e-01, 7.49749195e-01, 6.42994154e-01,\
        5.51439716e-01, 4.72921500e-01, 4.05583310e-01, 3.63174287e-01,\
        3.39542475e-01, 3.17448390e-01, 2.96791970e-01, 2.77479666e-01,\
        2.63840861e-01, 2.55143683e-01, 2.46733196e-01, 2.38599950e-01,\
        2.30734807e-01, 2.24707041e-01, 2.20384502e-01, 2.16145113e-01,\
        2.11987275e-01, 2.07909417e-01, 2.04728822e-01, 2.02406413e-01,\
        2.00110349e-01, 1.97840331e-01, 1.95596064e-01, 1.93377255e-01,\
        1.91183617e-01, 1.89014862e-01, 1.86870710e-01, 1.84750880e-01,\
        1.83027090e-01, 1.81688655e-01, 1.80360009e-01, 1.79041078e-01,\
        1.77731792e-01, 1.76432081e-01, 1.75141874e-01, 1.73861103e-01,\
        1.72589697e-01, 1.71327589e-01])
mu_water = mu_per_density_water * density_water


# mu = np.c_[mu_PMMA,mu_aluminum,mu_gadolinium].T

# mu = mu[0:nm,:]

# mu = np.c_[mu_bone, mu_poly, mu_acrylic, mu_air, mu_water].T
mu_air, mu_water, mu_iodine = mu_air/10, mu_water/10, mu_iodine/10
mu = np.c_[mu_air, mu_water, mu_iodine].T
mu = mu[0:nm,:]
mu = mu[:,:] #/ 10


def convert_to_HU(image):
    # Assume image contains the amount of PMMA at each pixel, since we are reconstructing assuming a single material that is PMMA
    # integrate over the source spectrum
    HU_image = np.zeros(image.shape[:2])  

    for energy_index in range(ni):
        mu_total = np.sum(image * mu[:, energy_index], axis=-1)  # (nx, ny)
        HU_image += spectrum[energy_index] * 1000 * (mu_total - mu_water[energy_index]) / (mu_water[energy_index] - mu_air[energy_index])

    return HU_image
def plot_x(x_list, title=None, filename=None, use_HU=True):
    vmin = np.ones(nm)*np.inf
    vmax = np.ones(nm)*-np.inf
    for x in x_list:
        # vmin = np.minimum(vmin,x.min((0,1)))
        # vmax = np.maximum(vmax,x.max((0,1)))
        vmin = -100
        vmax = 500
    for i in range(len(x_list)):
        plt.figure()
        if nm_algorithm == 1:
            if use_HU:
                plt.figure()
                plt.imshow(convert_to_HU(x_list[i]), cmap='gray', vmin = vmin, vmax = vmax)

            else:
                plt.imshow(x_list[i], cmap='gray')
            # plt.xlabel('PMMA')
            plt.xticks([])
            plt.yticks([])
            plt.title(f'{real_size}x{real_size}')
            plt.colorbar()
            plt.tight_layout()
        else:
            # x_np = np.array(x_list)
            print(x_list[1].shape)
            for channel, (material, _) in enumerate(materials.items()):
                plt.subplot(1,nm,channel+1)
                plt.imshow(x_list[i][:,:,channel],vmin=vmin[channel],vmax=vmax[channel], cmap='gray')
                plt.xlabel(f'{material}')
                plt.xticks([])
                plt.yticks([])
                plt.colorbar(shrink=0.25)
            
            plt.tight_layout()
            if(not(title is None)):
                plt.suptitle(title[i])
                plt.subplots_adjust(top=1.4)
        plt.savefig(filename[i])
        plt.close()


plt.plot(energies,mu_PMMA,label='PMMA')
plt.plot(energies,mu_aluminum,label='aluminum')
plt.plot(energies,mu_gadolinium,label='gadolinium')
plt.legend(loc='upper right')
plt.xlabel('energy (units: keV)')
plt.ylabel('mass attenuation coef. '+r'$\mu/\rho$'+' (units: cm'+r'${}^2$'+'/g)')
plt.yscale('log')
plt.savefig('material_spectra.jpg')
plt.close()

import tigre
import tigre.utilities.gpu as gpu

listGpuNames = gpu.getGpuNames()
if len(listGpuNames) == 0:
    print("Error: No gpu found")
else:
    for id in range(len(listGpuNames)):
        print("{}: {}".format(id, listGpuNames[id]))

gpuids = gpu.getGpuIds(listGpuNames[0]) 
gpuids.devices = gpus

print(gpuids)
real_size = phantom_size + 1
geo = tigre.geometry(mode="fan", nVoxel = np.array([1, nx, ny]))  # Set to parallel-beam geometry

geo.DSO = 625.61  # Source-to-Origin Distance
geo.DSD = 1097.6  # Source-to-Detector Distance (not used in parallel beam)
# Offset settings (default is 0, can be adjusted if necessary)
geo.offOrigin = np.array([0, 0, 0])  # Offset of the image center
geo.offDetector = np.array([0, 0])  # Offset of the detector position
geo.sVoxel = np.array([1, 220, 220])
geo.dVoxel = geo.sVoxel / geo.nVoxel
print('detector length' ,geo.DSD * np.tan(theta/4) * 2) # detector length
print('voxel size', geo.dVoxel)

geo2 = tigre.geometry(mode="fan", nVoxel = np.array([1, nx//2, ny//2]))
geo2.DSO = 625.61  # Source-to-Origin Distance
geo2.DSD = 1097.6  # Source-to-Detector Distance (not used in parallel beam)
# Offset settings (default is 0, can be adjusted if necessary)
geo2.offOrigin = np.array([0, 0, 0])  # Offset of the image center
geo2.offDetector = np.array([0, 0])  # Offset of the detector position
geo2.sVoxel = np.array([1, 220, 220])
geo2.dVoxel = geo2.sVoxel / geo2.nVoxel
save_angles = []


def load_or_create_P(ns, nu, nl, foldername, use_saved = True):
    global angles
    # nu = int(np.ceil(scanner_rad * theta / (image_side / nx)))  # Calculate the number of detector pixels
    geo.nDetector = np.array([1, nu])  # 1D detector with 1 Ã— nu pixels
    print(geo.DSO*theta/nu, 'detector')
    geo.dDetector = np.array([1.0915, 1.0965])  # Set the size of each detector pixel

    geo.sDetector = geo.nDetector * geo.dDetector  # total size of the detector    (mm)

    nangles = ns
    angles = np.linspace(0, 2 * np.pi, nangles, endpoint=False, dtype=np.float32)
    save_angles.append(angles)
    
    filename = os.path.join(foldername, f'P_{ns}_{nu}_{nl}.npy')
    if os.path.exists(filename) and use_saved:
        P = np.load(filename)
        print(f"Loaded P from {filename}")
    else:
        P = get_P(ns, nu, nl)
        # P1 = get_P1(ns, nu, nl)
        # print(np.mean((P-P1)**2), 'MSE')
        np.save(filename, P)
        print(f"Saved P to {filename}")
    return P


def load_or_create_P(ns, nu, nl, foldername, use_saved = True):
    global angles
    # global x_test
    # x_test = 12
    geo.nDetector = np.array([1, nu])  # number of pixels              (px)
    geo.dDetector = np.array([geo.dVoxel[0], 0.8])  # size of each pixel            (mm)
    geo.sDetector = geo.nDetector * geo.dDetector  # total size of the detector    (mm)
    # Offsets
    geo.offOrigin = np.array([0, 0, 0])  # Offset of image from origin   (mm)
    geo.offDetector = np.array([0, 0])  # Offset of Detector            (mm)

    nangles = ns
    angles = np.linspace(0, 2 * np.pi, nangles, endpoint=False, dtype=np.float32)
    save_angles.append(angles)

def load_or_create_S(total_intensity, nl, foldername, use_saved = True):    
    S = get_S(total_intensity=total_intensity, nl=nl)
    return S

def load_or_create_c(x_true, nl, S, seed, use_noise, foldername, use_saved = True):
    c = get_c(x_true, nl, S, seed=seed, use_noise=use_noise)
    return c

def load_or_create_L(P, S, mu_PMMA, nl, nx, ny, foldername, use_saved = True):
    filename = os.path.join(foldername, f'L_{nl}_{nx}_{ny}.npy')
    if os.path.exists(filename) and use_saved:
        L = np.load(filename)
        print(f"Loaded L from {filename}")
    else:
        # Compute Pmat
        Pmat = P.reshape((nl, nx * ny))  # [nl, nk]
        nj = len(mu_PMMA)
        maxoverw = 0

        for w in tqdm(range(S.shape[0]),desc='Calculating L'):
            partial_sum = 0
            for j in range(nj):
                partial_sum += S[w, :, j] * mu_PMMA[j]
            val = np.max(partial_sum.flatten())  # Using max variation across rays
            if val > maxoverw:
                maxoverw = val

        # Compute lambdamax
        lambdamax = np.linalg.norm(Pmat.T @ Pmat, ord=2)  # Largest eigenvalue of P.T @ P
        print(f'lambdamax is {lambdamax}')

        # Compute L
        L = lambdamax * maxoverw / nl
        np.save(filename, L)
        print(f"Saved L to {filename}")

    return L

def get_P1(ns, nu, nl):
    # Create projection matrix
    P = np.zeros((ns, nu, nx, ny))
    pixel_xs = -image_side / 2 + image_side * np.arange(1 + nx) / nx
    pixel_ys = -image_side / 2 + image_side * np.arange(1 + ny) / ny

    detector_pixel_sizes = []  # Store detector pixel sizes

    for s_ in tqdm(range(ns), desc="calculating P"):
        source_angle = s_ * 2 * np.pi / float(ns)
        source = np.r_[scanner_rad * np.cos(source_angle), scanner_rad * np.sin(source_angle)]

        # Compute detector pixel size
        detector_positions = []
        for u_ in range(nu):
            detector_angle = source_angle + np.pi - (0.5 - 0.5 / nu) * theta + u_ * theta / float(nu)
            detector = np.r_[scanner_rad * np.cos(detector_angle), scanner_rad * np.sin(detector_angle)]
            detector_positions.append(detector)

        # Compute distances between consecutive detector pixels
        for u_ in range(nu - 1):
            det_size = np.linalg.norm(detector_positions[u_ + 1] - detector_positions[u_])
            detector_pixel_sizes.append(det_size)

        for u_ in range(nu):
            detector = detector_positions[u_]

            t_vals = np.zeros(0)
            if source[0] != detector[0]:
                t_vals = np.r_[t_vals, (pixel_xs - source[0]) / (detector[0] - source[0])]
            if source[1] != detector[1]:
                t_vals = np.r_[t_vals, (pixel_ys - source[1]) / (detector[1] - source[1])]
            t_vals = np.sort(np.unique(t_vals))
            t_vals = np.r_[0, t_vals[(0 < t_vals) & (t_vals < 1)], 1]

            length = np.sqrt(((source - detector) ** 2).sum())

            for it in range(len(t_vals) - 1):
                t = (t_vals[it] + t_vals[it + 1]) / 2
                x_coord = (1 - t) * source[0] + t * detector[0]
                y_coord = (1 - t) * source[1] + t * detector[1]
                if (pixel_xs[0] <= x_coord) & (x_coord <= pixel_xs[nx]) & (pixel_ys[0] <= y_coord) & (
                        y_coord <= pixel_ys[ny]):
                    ix = np.argwhere(pixel_xs <= x_coord).max()
                    iy = np.argwhere(pixel_ys <= y_coord).max()
                    P[s_, u_, ix, iy] = P[s_, u_, ix, iy] + length * (t_vals[it + 1] - t_vals[it])

    P = P.reshape((nl, nx, ny))

    # Compute and print average detector pixel size
    avg_detector_pixel_size = np.mean(detector_pixel_sizes)
    print(f"Average detector pixel size: {avg_detector_pixel_size:.4f} cm")

    return P

def get_P(ns, nu, nl):
    A_cols = []
    n_voxels = real_size * real_size  # or 43*43
    for i in tqdm(range(n_voxels)):
        x_basis = np.zeros((1, real_size, real_size), dtype=np.float32)
        x_basis.flat[i] = 1.0  # set a single voxel to 1
        proj = tigre.Ax(x_basis, geo, angles, gpuids=gpuids)  # (n_angles, 1, detector_pixels)
        A_cols.append(proj.reshape(-1))  # flatten sinogram and store as column
    A_matrix = np.stack(A_cols, axis=1)  # shape: (n_proj_pixels, n_voxels)
    A_matrix = A_matrix.reshape(-1, real_size, real_size)
    print('A shape', A_matrix.shape)
    return A_matrix
# print(x_test)
if len(save_angles) != 0:
    global angles
    angles = save_angles[-1]
    print('saved angles')

def Pmult(x, P, nl, nm = nm): # apply projection operator to an image x
    Px = np.zeros((nl,nm))
    for m_ in range(nm):
        Px[:,m_] = (P*x[:,:,m_]).sum((1,2))
    return Px

def Ptmult(y, P): # transpose of the projection operator
    return np.dot(P.transpose((1,2,0)),y)

def get_Psums(P, nl):
    P_rowsums = Pmult(x=np.ones((nx,ny,nm)), P=P, nl=nl)[:,0]
    P_rowsums[P_rowsums<div_by_zero_thresh] = div_by_zero_thresh # avoiding dividing by zero
    P_colsums = Ptmult(y=np.ones((nl,nm)), P=P)[:,:,0]
    P_colsums[P_colsums<div_by_zero_thresh] = div_by_zero_thresh # avoiding dividing by zero
    return P_rowsums, P_colsums


# Approximate exponential, for numerical stability
def qexp(t,deriv=0):
    return np.exp(t*(t<=0))*(t<=0) + \
            (1+t+0.5*t**2)*((t>0)&(deriv==0)) + (1+t)*((t>0)&(deriv==1)) + ((t>0)&(deriv==2))


# Forward model
def get_c_hat_by_energy(y, S, deriv=0, mu = mu):
    # c_hat[w,l,i] = E[# photons in (w,l) at energy i]
    if y.shape[1] == 1: #nm_algorithm
        mu = mu[2][None,...]
        # mu = np.sum(mu, axis = 0)[None,...] #np.mean converge after 34000 steps, also really slow
        
    elif y.shape[1] == nm:
        pass
    # print(f'mu {mu.shape}, y {y.shape}, dot {np.dot(y,mu).shape}')
    return (S * qexp(-np.dot(y,mu),deriv))

def get_c_hat(y, S, deriv=0):
    # c_hat[w,l] = E[# photons in (w,l), across all energies]
    return get_c_hat_by_energy(y=y, S=S, deriv=deriv).sum(2)

def get_c_hat_by_energy_div(y, S, deriv=0, mu = mu):
    # c_hat[w,l,i] = E[# photons in (w,l) at energy i]
    if y.shape[1] == 1: #nm_algorithm
        mu = mu[2][None,...]
        # mu = np.sum(mu, axis = 0)[None,...] #np.mean converge after 34000 steps, also really slow
        
    elif y.shape[1] == nm:
        pass
    print(qexp(-np.dot(y,mu),deriv).shape, S.shape)
    return (qexp(-np.dot(y,mu),deriv))

def get_c_hat_div(y, S, deriv=0):
    # c_hat[w,l] = E[# photons in (w,l), across all energies]
    print(get_c_hat_by_energy(y=y, S=S, deriv=deriv).shape, get_c_hat_by_energy_div(y=y, S=S, deriv=deriv).shape)
    return (get_c_hat_by_energy_div(y=y, S=S, deriv=deriv)).sum(1)[None,...]

def get_c(x_true, nl, S, seed, use_noise):
    temp = None
    for i in range(nm):
        temp = tigre.Ax(x_true[...,i][None,...], geo, angles, gpuids=gpuids)
        if i == 0:
            y = temp
        else:
            y = np.concatenate([y, temp], axis = 1)
    
    y = y.transpose(0,2,1).reshape(-1, 3)
    if use_noise:
        np.random.seed(seed)
        # c = np.random.poisson(get_c_hat(y=Pmult(x=x_true, P=P, nl=nl), S=S)) # generate data

        c = np.random.poisson(get_c_hat(y, S=S)) # generate data
    else:
        c = get_c_hat(y, S=S)
    print(c.shape)
    return c

# x_bar is a global variable
# x should be only mu_iodine
# but when we use c_star, we want to reconstruct the whole scene so use full mus


# Monotone operator
def get_monotone_direction(x, c_star, nl, S):
    # x is shape [nx, ny, nm], Pmult(x) is shape [nl, nm]
    # assert x.shape[-1] == 1, f'monotone operator only works for single-material models'
    x_bar[..., -1] = x[..., -1]
    x = x_bar
    temp = None
    for i in range(nm):
        temp = tigre.Ax(x[...,i][None,...], geo, angles, gpuids=gpuids)
        if i == 0:
            y = temp
            tigre_shape = temp.shape
            # print(tigre_shape)
        else:
            y = np.concatenate([y, temp], axis = 1)
            # print('y', y.shape)
    
    y = y.transpose(0,2,1).reshape(-1, 3)
    # print(y.shape)
    temp = get_c_hat(y, S=S)
    # print(temp.shape)
    error = c_star - temp  
    result = None
    # print(error.shape)
    for i in range(nw):
        results_temp = tigre.Atb(np.array(error[i].reshape(tigre_shape), dtype = np.float32),geo,angles,backprojection_type="matched",gpuids=gpuids)
        # print(results_temp.shape)
        if i == 0:
            result = results_temp
        else:
            result = np.concatenate([result, results_temp])
    result = result.sum(axis=0).reshape(nx, ny, nm_algorithm)  # [nx, ny, nm]
    result = result / nl  
    return result

# Gradient descent on MSE loss (L2 error minimization)
def get_L2_direction(x,c,S,P,nl):
    c_hat = get_c_hat(y=Pmult(x=x,P=P,nl=nl), S=S)
    error = c - c_hat  # [nw, nl]
    a = P.reshape(nl, -1)  # [nl, nx*ny]
    expterm = qexp(-np.dot(Pmult(x,P,nl),mu))  # [nl, ni]
    direction = 0
    for w in range(nw):
        for l in range(nl):
            direction = direction + error[w,l] * a[l].reshape(nx*ny,1) @ (expterm[l].reshape(1, ni) @ np.diag(S[w,l,:]) @ mu.T)  # [nx*ny, nm]
    return direction.reshape(nx, ny, nm) / nl


# L1 loss
def get_L1loss(x,c,S,P,nl):
    c_hat = get_c_hat(y=Pmult(x=x,P=P,nl=nl), S=S)  # [nw, nl]
    return np.sum(np.abs(c - c_hat).flatten()) / nl


# Poisson negative log likelihood
def get_loss(x,c,S,P,nl):
    # poisson negative log likelihood, up to a constant term
    #   ( this constant term makes it nonnegative, & ensures that if c = c_hat then loss = 0 )
    c_hat = get_c_hat(y=Pmult(x=x,P=P,nl=nl), S=S)
    if min(c_hat[c>0])<=0:
        return np.inf
    else:
        return (c_hat.sum() - c.sum()) - (c[c>0]*np.log(c_hat[c>0]/c[c>0])).sum()

def get_plot_curves(x_iters, c, S, nl, x_true, method='admm'):
    max_iters = len(x_iters)
    if np.min(x_iters) < 0:
        print(f'{method} has negative values in x_iters: {np.min(x_iters)}')
    # choose a subset of the iters to actually evaluate, to speed things up
    iters = np.arange(np.log(max_iters), step=0.01)
    iters = [0] + [int(it) for it in np.exp(iters)] + [max_iters - 1]
    iters = np.unique(iters)
    itersminus1 = np.array([max(0, it-1) for it in iters])
    avgiters = [np.mean(x_iters[int(it/2):it+1], axis=0) for it in iters]
    avgitersminus1 = [np.mean(x_iters[int(it/2):it+1], axis=0) for it in itersminus1]
    normdiff = np.array([np.linalg.norm(avgiters[it] - avgitersminus1[it]) for it in range(len(iters))])
    # for each iter, evaluate loss and RMSE for the iterate itself, and averaged over the last t/2 iterates
    if method == 'admm':
        loss = np.array([get_loss(x=x,c=c,S=S,P=P,nl=nl) for x in x_iters[iters]])
        avgloss = np.array([get_loss(x=x,c=c,S=S,P=P,nl=nl) for x in avgiters])
    elif method == 'polyaksgm':
        loss = np.array([get_L1loss(x=x,c=c,S=S,P=P,nl=nl) for x in x_iters[iters]])
        avgloss = np.array([get_L1loss(x=x,c=c,S=S,P=P,nl=nl) for x in avgiters])
    elif method == 'monotone' or method == 'extragradient':
        loss = np.array([np.linalg.norm(get_monotone_direction(x=x,c_star=c,nl=nl,S=S))/nk for x in x_iters[iters]])
        avgloss = np.array([np.linalg.norm(get_monotone_direction(x=x,c_star=c,nl=nl,S=S))/nk for x in avgiters])
    elif method == 'mse_gd':
        loss = np.array([np.linalg.norm(get_L2_direction(x=x,c=c,P=P,nl=nl,S=S))/nk for x in x_iters[iters]])
        avgloss = np.array([np.linalg.norm(get_L2_direction(x=x,c=c,P=P,nl=nl,S=S))/nk for x in avgiters])
    rmse = np.array([np.sqrt(np.mean((x - x_true)**2)) for x in x_iters[iters]])
    avgrmse = np.array([np.sqrt(np.mean((x - x_true)**2)) for x in avgiters])
    return iters, loss, rmse, normdiff, avgloss, avgrmse #, avgnormratio
       

################
# TV projection
################


def TV(image):
    grad_x = np.roll(image, -1, axis=1) - image
    grad_y = np.roll(image, -1, axis=0) - image
    return np.sum(np.sqrt(grad_x**2 + grad_y**2))

oracle_tv = TV(x_true[...,-1])
print('TV oracle', oracle_tv, 'TV for whole channel', TV(x_true))
# oracle_tv = TV(x_true)

# https://en.wikipedia.org/wiki/Dykstra%27s_projection_algorithm
# Project onto intersection of two convex constraint sets, 
# in this case the positive orthant and the TV ball
def project_tv_nonnegative(image, max_tv=oracle_tv, w_guess=0.5):
    x = image
    p = 0
    q = 0
    iters = 0
    while True:
        iters = iters + 1
        oldx = x
        y = np.clip(x + p, a_min=0, a_max=None)
        p = x + p - y
        x, w_guess = project_tv(y + q, max_tv=max_tv, w_guess=w_guess)
        q = y + q - x
        # print('inside tv')
        if iters % 100 == 0:
            print(f'at iter {iters}: convergence criterion is {np.linalg.norm(x - oldx)}')
        if np.linalg.norm(x - oldx) <= 1e-4:  # was 1e-8, but can be extremely slow to converge
            return x, w_guess


def project_tv(image, max_tv=oracle_tv, w_guess=1):
    # First project the image to the nonnegative orthant
    # image = np.clip(image, a_min=0, a_max=None)
    I = np.copy(image)
    # assert np.min(I) >= 0, f'negative values in image {I}'
    # maxtv=0 is a default to denote no TV projection (unregularized)
    max_tv_temp = []
    # for i in range(nm):
    #     max_tv_temp.append(TV(I[..., i]))
    # print(max_tv_temp, TV(I), I.shape)
    if max_tv == 0:
        return I, w_guess
    # check if the unprojected image already satisfies maxtv
    if TV(I) <= max_tv:
        return I, w_guess
    w = w_guess * 2
    count = 0
    lower = 0
    upper = np.inf
    while True:
        count = count + 1
        # print('before prox')
        I = prox_tv.tv1_2d(x=image, w=w)
        # print('before TV')
        tv = TV(I)
        # print(f'iter {count} has TV {tv} with w {w}. criterion is {np.abs(tv - max_tv) / max_tv}, maxtv is {max_tv}. range is [{lower}, {upper}].')
        if count > 100:
            assert False
        if np.abs(tv - max_tv) / max_tv < 0.01:
            return I, w
        if tv > max_tv:
            lower = w
            w = np.min([w * 10, 0.5 * (lower + upper)])
        if tv < max_tv:
            upper = w
            w = 0.5 * (lower + upper)


class RunningAverage:
    def __init__(self):
        self.iterates = []
        self.prefix_sum = [0]  # Prefix sum where prefix_sum[i] stores sum of iterates[0] to iterates[i-1]

    def update(self, x_t):
        self.iterates.append(x_t)
        self.prefix_sum.append(self.prefix_sum[-1] + x_t)

    def replaceprev(self, x_t):
        if len(self.iterates) == 0:
            raise ValueError("No iterates to replace")
        self.prefix_sum[-1] = self.prefix_sum[-1] - self.iterates[-1] + x_t
        self.iterates[-1] = x_t

    def get_xbar(self, t):
        if t > len(self.iterates):
            raise ValueError("t exceeds the number of stored iterates")

        sum_last_half = self.prefix_sum[t] - self.prefix_sum[t // 2]
        return sum_last_half / (t - t // 2)
    
    def get_x(self, t):
        if t > len(self.iterates):
            raise ValueError("t exceeds the number of stored iterates")

        return self.iterates[t]
    

color_dict = {'monotone': 'xkcd:purple', 
              'admm': 'xkcd:orange', 
              'polyaksgm': 'xkcd:green', 
              'extragradient': 'xkcd:blue', 
              'msegd': 'xkcd:dark pink'}
shape_dict = {'monotone': 'P', 
              'admm': 'v',
              'polyaksgm': 'X',
              'extragradient': 'o',
              'msegd': 'D'}


import os
import random
import numpy as np
def seed_everything(seed: int = 42):
    # 1. Python built-in
    random.seed(seed)

    # 2. NumPy
    np.random.seed(seed)

    # 5. Environment variables (optional but safer)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # CUDA 10.2+

    # 6. If using DataLoader with multiple workers
    def seed_worker(worker_id):
        worker_seed = seed + worker_id
        np.random.seed(worker_seed)
        random.seed(worker_seed)

    return seed_worker
